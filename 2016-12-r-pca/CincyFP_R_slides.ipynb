{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Brief Introduction to R & Feature Transformation\n",
    "## Chris Hodapp <hodapp87@gmail.com>\n",
    "\n",
    "## [CincyFP](https://cincyfp.wordpress.com/), 2016 December 13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Front matter\n",
    "\n",
    "This is all done in Jupyter (formerly IPython) and IRkernel.\n",
    "- https://jupyter.org/\n",
    "- https://irkernel.github.io/\n",
    "\n",
    "Visit http://159.203.72.130:8888 to use this same notebook in your browser.\n",
    "\n",
    "(...unless you're reading this later, of course.  Go fire up your own docker container with `\"docker run -d -p 8888:8888 jupyter/r-notebook\"` or something.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](r-matey.png)\n",
    "\n",
    "(thanks Creighton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is R?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- An interpreted, dynamically-typed language based on S and made mainly for interactive use in statistics and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Sort of like MATLAB, except statistics-flavored and open source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A train-wreck that is sometimes confused with a real programming language.\n",
    "  - \"R is a dynamic language for statistical computing that combines lazy functional features and object-oriented programming. This rather unlikely linguistic cocktail would probably never have been prepared by computer scientists, yet the language has become surprisingly popular.\" *(Evaluating the Design of the R Language)*\n",
    "  - The R Inferno (Patrick Burns), http://www.burns-stat.com/pages/Tutor/R_inferno.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## So... why use it at all?\n",
    "\n",
    "- Stable and documented extensively!\n",
    "- Excellent for exploratory use interactively!\n",
    "- Epic visualization!\n",
    "- Magical, fast, and elegant for arrays, tables, vectors, and linear algebra!\n",
    "- Huge standard library!\n",
    "- Packages for everything else on CRAN!\n",
    "- Still sort of FP!\n",
    "- Excellent tooling! (Sweave, Emacs & ESS mode, RStudio, Jupyter...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do I use R?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Do you need plotting or visualization?*\n",
    "Use [ggplot2](http://ggplot2.org/). Completely ignore built-in plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Do you need to transform tabular/vector/list/array/matrix/DataFrame data somehow?*\n",
    "Just use [dplyr](https://cran.r-project.org/package=dplyr) or [reshape2](http://seananderson.ca/2013/10/19/reshape.html). Completely ignore built in `*apply` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Do you need something else?* Search [CRAN](https://cran.r-project.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*Does no CRAN package solve your problems? Do you need to write \"real\"(tm) software for production?* Strongly consider giving up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Obligatory R notebook demonstration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(dplyr)\n",
    "# Arithmetic\n",
    "# Variables\n",
    "# Function example\n",
    "# Show help example\n",
    "c(0,1,2,3,4)\n",
    "0:10\n",
    "seq(0,10,0.2)\n",
    "x <- 1:10\n",
    "x + 5\n",
    "x + x\n",
    "x[c(1,2)]\n",
    "x < 5\n",
    "x[x < 5]\n",
    "sum(x)\n",
    "x*10\n",
    "x + 10\n",
    "x * x\n",
    "mean(x)\n",
    "c(0,1,2,3,4,5,6,NA,NA,10)\n",
    "prod(c(1,2,3,4,5,6,NA,NA,10), na.rm = TRUE)\n",
    "# NA != NaN != Inf:\n",
    "c(sqrt(-1), 1/0)\n",
    "# Named indices:\n",
    "pt <- c(10,20,30)\n",
    "names(pt) <- c(\"x\", \"y\", \"z\")\n",
    "pt[\"x\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataframes\n",
    "\n",
    "- A sort of blending of matrices, arrays, and database tables.  Type is per-column.\n",
    "- Accessed by row range, column range, or both.\n",
    "\n",
    "### Many imitators:\n",
    "- *Python*: Pandas, http://pandas.pydata.org/\n",
    "  - Stricter indexing (oriented for time-series)\n",
    "- *Scala, Java, Python*: [Apache Spark DataFrame](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes)\n",
    "  - Tied to Spark SQL & Catalyst; weakly-typed distributed `Dataset`\n",
    "- *Haskell*: Frames, https://github.com/acowley/Frames\n",
    "- *Clojure*: Incanter (?), http://incanter.org/\n",
    "- *Julia*: https://github.com/JuliaStats/DataFrames.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "df <- data.frame(a = 1:10, b = 0, c = 0)\n",
    "# Broadcasting (from above)\n",
    "df$d <- 6\n",
    "# Column access:\n",
    "df[,c(\"a\", \"b\")]\n",
    "# Row access:\n",
    "df[4:7,]\n",
    "# Both:\n",
    "df[c(5,6), c(\"c\", \"d\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## dplyr\n",
    "\n",
    "- See: Introduction to dplyr, https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html\n",
    "- `filter`, `slice` - Select rows (filter is by predicate, slice is by position)\n",
    "- `arrange` - Reorder rows\n",
    "- `select`, `rename` - Select columns\n",
    "- `distinct` - Choose only *distinct* rows\n",
    "- `mutate`, `transmute` - Make new columns from existing ones\n",
    "- `summarise` - 'Peel off' one grouping level (or collapse frame to one row) with aggregate functions\n",
    "- `sample_n`, `sample_frac` - Randomly sample (by count or by percentage)\n",
    "- `group_by` - Group observations (most of above worked on grouped observations)\n",
    "- All dplyr calls are side-effect-free, and `x %>% f(y) = f(x,y)`\n",
    "- Bonus: This all works on remote SQL databases too (and `data.table` via `dtplyr`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "? mtcars\n",
    "mtcars2 <- tibble::rownames_to_column(mtcars) %>% rename(car= rowname)\n",
    "filter(mtcars2, gear == 4 & am == 1)\n",
    "arrange(mtcars2, cyl, mpg) %>% select(car, cyl, mpg, disp)\n",
    "transform(mtcars2, pw = hp / wt) %>%\n",
    "    arrange(-pw) %>%\n",
    "    select(car, pw, mpg) %>%\n",
    "    slice(1:10)\n",
    "distinct(mtcars2, cyl)\n",
    "group_by(mtcars2, cyl) %>%\n",
    "    summarise(count = n(),\n",
    "              avg_mpg = mean(mpg),\n",
    "              avg_hp = mean(hp),\n",
    "              avg_disp = mean(disp))\n",
    "ggplot(mtcars2, aes(x = hp, y = mpg, group = cyl)) +\n",
    "    geom_point(aes(colour = as.factor(cyl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivating Example\n",
    "\n",
    "- Example data set from: https://archive.ics.uci.edu/ml/datasets/Letter+Recognition\n",
    "- 20,000 samples, 16 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Load this as it is used throughout\n",
    "# the next section\n",
    "letters <- read.table(\"letter-recognition.data\", sep=\",\", header=FALSE);\n",
    "colnames(letters) <- c(\"Letter\", \"Xbox\", \"Ybox\", \"Width\", \"Height\",\n",
    "                       \"OnPix\", \"Xbar\", \"Ybar\", \"X2bar\", \"Y2bar\",\n",
    "                       \"XYbar\", \"X2Ybar\", \"XY2bar\", \"Xedge\",\n",
    "                       \"XedgeXY\", \"Yedge\", \"YedgeYX\");\n",
    "head(letters)\n",
    "dim(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unsupervised Learning\n",
    "\n",
    "- Sometimes a precursor to supervised learning\n",
    "- Sometimes done for its own sake\n",
    "- Some broad (and overlapping) categories:\n",
    "  - Latent variable models\n",
    "  - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Contrast: Most ML talks so far have been on supervised learning.\n",
    "- *Hidden structure* of the data\n",
    "- k-means, and EM is generalization\n",
    "- Overlap: EM is both categories\n",
    "- Curse of Dimensionality: why UL may be needed prior to SL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## *Curse of Dimensionality* (Bellman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- same Bellman behind Bellman equation that is basis of control theory, RL, and a lot of AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](./wikimedia_Data3classes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](./wikimedia_Map1NN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Intuition from k-nearest neighbor: If each sample occupies a certain amount of 'space' in the input space, the number of samples required to still 'cover' that space increases exponentially with the number of dimensions. *(That's the hand-waving description. See Vapnik-Chervonenkis dimension and Computational Learning Theory.)*\n",
    "- If possible: Don't add more dimensions. Either reduce dimensions, or increase samples.\n",
    "- But... how to remove dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Why feature transformation: how do we reduce dimensions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature Transformation\n",
    "\n",
    "### General form\n",
    "\n",
    "$$x : \\mathcal{F}^N \\mapsto \\mathcal{F}^M, M < N$$\n",
    "\n",
    "*(though actually `M >> N` is useful too and is the basis for [kernel methods](https://en.wikipedia.or/wiki/Kernel_method) such as SVMs)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Kernel trick, dot product\n",
    "- Feature selection = simpler form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Subsets\n",
    "- *Feature Selection*: Loosely, throw away dimensions/features.\n",
    "- Information gain, Gini index, entropy, variance, statistical independence...\n",
    "- *Filtering*: Reduce features first, and then perform learning. Learning can't feed information 'back' to filtering.\n",
    "- *Wrapping*: Reduce features based on how learning performs.\n",
    "  - *Forward search:* Add features one by one. At each step, add the feature that helps the learner the most.\n",
    "  - *Backward search:* Remove features one by one. At each step, discard the feature that impacts the learner the least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Decision trees are sort of a form of wrapping\n",
    "- \"throw dimension away, check if it changed\" is plenty viable actually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Relevance & Usefulness\n",
    "- *Usefulness:* How useful is this feature to a specific learner?\n",
    "- *Relevance:* What is the theoretical amount of information in this feature?\n",
    "  - Relates to *Bayes Optimal Classifier* (BOC): What is the theoretical best we can do given this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- Everything else than BOC has *bias*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- *Strongly relevant:* Removing the feature degrades BOC\n",
    "- *Weakly relevant:* For every subset of features, adding this feature improves BOC\n",
    "  - Adding a feature may turn a strongly relevant one to weakly relevant.\n",
    "- *Irrelevant:* All other cases\n",
    "  - *Irrelevant* features may still be *useful* - on some learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Subsets\n",
    "\n",
    "#### Linear\n",
    "\n",
    "- Transformation $x : \\mathcal{F}^N \\mapsto \\mathcal{F}^M, M < N$ is defined by $N\\times M \\textrm{ matrix }\\mathcal{P}_x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- e.g. 4-dimensional feature space mapped to 2 dimensions, $(x_1, x_2, x_3, x_4) \\mapsto (2x_1-x_2, x_3 + x_4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then for $X$ containing samples as row vectors, $Y=X\\mathcal{P}_x$:\n",
    "$$\n",
    "\\mathcal{P}_x=\n",
    "  \\begin{bmatrix}\n",
    "    2 & 0 \\\\\n",
    "    -1 & 0 \\\\\n",
    "    0 & 1 \\\\\n",
    "    0 & 1\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Take the above type (F^N->F^M), and make more explicit/algebraic\n",
    "- Bear with me, might be math-heavy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Consider data expressed as an $n\\times m$ matrix with each column representing one *feature* (of $m$) and each row one *sample* (of $n$):\n",
    "\n",
    "$$\n",
    "X=\n",
    "  \\begin{bmatrix}\n",
    "    a_1 & b_1 & c_1 & \\cdots\\\\\n",
    "    a_2 & b_2 & c_2 & \\cdots\\\\\n",
    "    a_3 & b_3 & c_3 & \\cdots\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots\\\\\n",
    "    a_n & b_n & c_n & \\cdots\\\\\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Show letters data here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Focus on first feature $A=\\left\\{a_1, a_2, \\dots\\right\\}$\n",
    "- Mean = $\\left\\langle a_i \\right\\rangle_i = \\frac{1}{n} \\sum_i^n a_i=\\mu_A$\n",
    "  - $\\left\\langle \\dots \\right\\rangle$ = expectation operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Variance:\n",
    "$$\\sigma_A^2=\\left\\langle \\left(a_i-\\left\\langle a_j \\right\\rangle _j\\right)^2 \\right\\rangle_i=\\left\\langle \\left(a_i-\\mu_A\\right)^2 \\right\\rangle_i = \\frac{1}{n-1}\\sum_i^n \\left(a_i-\\mu_A\\right)^2$$\n",
    "\n",
    "*(if you want to know why it is $\\frac{1}{n-1}$ and not $\\frac{1}{n}$, ask a statistics PhD or something)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Consider another feature $B=\\left\\{b_1, b_2, \\dots\\right\\}$, and assume that $\\mu_A=\\mu_B=0$ for sanity\n",
    "- Covariance of $A$ and $B$:\n",
    "$$\\sigma_{AB}^2=\\left\\langle a_i b_i \\right\\rangle_i=\\frac{1}{n-1}\\sum_i^n a_i b_i$$\n",
    "- If $\\sigma_A^2=\\sigma_B^2=1$ then $\\sigma_{AB}^2=\\rho_{A,B}$ (correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "- Covariance is where this starts to tell us something about our dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Treating $A$ and $B$ as vectors:\n",
    "\n",
    "$$\\sigma_{AB}^2=\\frac{A\\cdot B}{n-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Recalling our data matrix:\n",
    "\n",
    "$$\n",
    "X=\n",
    "  \\begin{bmatrix}\n",
    "    a_1 & b_1 & c_1 & \\cdots\\\\\n",
    "    a_2 & b_2 & c_2 & \\cdots\\\\\n",
    "    a_3 & b_3 & c_3 & \\cdots\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots\\\\\n",
    "    a_n & b_n & c_n & \\cdots\\\\\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It can be rewritten as column vectors:\n",
    "\n",
    "$$\n",
    "X=\n",
    "  \\begin{bmatrix}\n",
    "    a_1 & b_1 & c_1 & \\cdots\\\\\n",
    "    a_2 & b_2 & c_2 & \\cdots\\\\\n",
    "    a_3 & b_3 & c_3 & \\cdots\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots\\\\\n",
    "    a_n & b_n & c_n & \\cdots\\\\\n",
    "  \\end{bmatrix}\n",
    "  =\\begin{bmatrix}\n",
    "  A & B & C & \\cdots\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Then *covariance matrix* is:\n",
    "\n",
    "$$\\mathbf{S}_X=\\frac{X^\\top X}{n-1}=\n",
    "  \\begin{bmatrix}\n",
    "    \\sigma_{A}^2 & \\sigma_{AB}^2 & \\sigma_{AC}^2 & \\sigma_{AD}^2 & \\cdots \\\\\n",
    "    \\sigma_{AB}^2 & \\sigma_{B}^2 & \\sigma_{BC}^2 & \\sigma_{BD}^2 & \\cdots \\\\\n",
    "    \\sigma_{AC}^2 & \\sigma_{BC}^2 & \\sigma_{C}^2 & \\sigma_{CD}^2 & \\cdots \\\\\n",
    "    \\sigma_{AD}^2 & \\sigma_{BD}^2 & \\sigma_{CD}^2 & \\sigma_{D}^2 & \\cdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Square ($m \\times m$), symmetric, variances on diagonals, covariances off diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "mtx <- scale(select(letters, -Letter))\n",
    "cm <- (t(mtx) %*% mtx) / (nrow(mtx) + 1)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If all features are completely independent of each other, all covariances are 0.\n",
    "- That is: Covariance matrix is a *diagonal matrix* (all zeros, except for diagonal).\n",
    "- So... What is this matrix $P$ such that for $Y=XP$, covariance matrix $\\mathbf{S}_Y$ is diagonal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Like basically every other question in linear algebra, the answers are:\n",
    "  - Eigendecomposition\n",
    "  - SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- That magical transform matrix $P$ equals a matrix whose columns are eigenvectors of $X^\\top X$. (Left as an exercise for the reader.)  Since covariance matrix $X^\\top X$ is a symmetric and positive semidefinite matrix, its eigenvectors form an orthogonal basis with non-negative eigenvalues (obviously).\n",
    "- Eigenvectors are the *principal components* of $X$.\n",
    "- Corresponding eigenvalues are the variance of $X$ 'along' each component (also equal to the diagonals of $\\mathbf{S}_Y$) - or the 'variance explained' by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Roughly, this means eigenvector forms a\n",
    "# coordinate transform matrix.\n",
    "r <- eigen(cm)\n",
    "r$values\n",
    "r$vectors\n",
    "dim(r$vectors)\n",
    "lettersPca <- mtx %*% r$vectors\n",
    "round(t(lettersPca) %*% lettersPca / (nrow(lettersPca) + 1), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "- $P$ is an $m\\times m$ matrix. Every column is a component, and assume they are in descending order of variance (almost all eigendecomposition implementations do this regardless).\n",
    "- Data matrix $X$ is $n \\times m$.\n",
    "- $Y=XP$ then is also $n \\times m$, and is the result of projecting each sample in $X$ onto each component.\n",
    "  - In effect, a coordinate transform\n",
    "  - A reversible one; since $P$ is an orthogonal basis, $P^{-1}=P^\\top$, $X=YP^\\top$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- $Y$ is in a new space, still $m$-dimensional.\n",
    "  - 1st column = 1st dimension = $X$'s projection on most important component\n",
    "  - 2nd column = 2nd dimension = $X$'s projection on 2nd most important component\n",
    "  - and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Two equivalent ways of reducing dimensionality:\n",
    "  - Throw away \"less important\" dimensions from the end of $Y$, e.g. let $Y_d$ = the 1st $d$ columns of $Y$ ($n\\times d$ matrix)\n",
    "  - Or, let $P_d$ equal the first $d$ columns of $P$ ($m\\times d$ matrix), then $Y_d=XP_d$\n",
    "- Rule of thumb: Keep enough dimensions for 90% of total variance\n",
    "- 'Lossy' reconstruction back into original $m$-dimensional space: $\\hat{X}=Y_dP_d^\\top$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# It's provable that this provides\n",
    "# the \"best\" reconstruction (by\n",
    "# reconstruction error)\n",
    "cumsum(r$values) / sum(r$values)\n",
    "\n",
    "lettersDf <- data.frame(class = letters$Letter, pca = lettersPca)\n",
    "head(lettersDf)\n",
    "decim <- sample_frac(lettersDf, 0.025)\n",
    "ggplot(decim, aes(label = class, pca.1, pca.2, colour = class)) + geom_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "- We have thus just derived (in abbreviated fashion) a ridiculously useful tool called PCA (Principle Component Analysis).\n",
    "  - It's often done with SVD rather than eigendecomposition (better-behaved numerically, provides other information)\n",
    "  - It is a linear algebra method that tries to find uncorrelated Gaussians.  Uncorrelated sometimes coincides with statistically independent.\n",
    "  - The almost-completely-unrelated *ICA (Independent Component Analysis)* derives independent features using probability and information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Projections / RCA\n",
    "\n",
    "- This is a stupid, stupid algorithm that shouldn't work:\n",
    "  1. Pick $m$ random directions in the $n$-dimensional space, $m < n$.\n",
    "  2. Project the $n$-dimensional data onto them.\n",
    "  3. Is the projection good enough (e.g. low reprojection error)?\n",
    "     - Yes: You're done.\n",
    "     - No: Repeat step 1.\n",
    "- It does work - very quickly, and irritatingly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References Used\n",
    "\n",
    "- Official R intro: https://cran.r-project.org/doc/manuals/R-intro.html\n",
    "- Evaluating the Design of the R Language (Morandat, Hill, Osvald, Vitek): http://r.cs.purdue.edu/pub/ecoop12.pdf\n",
    "- PCA Tutorial: http://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf\n",
    "- Machine Learning (Mitchell)\n",
    "- Ch. 11, Mining of Massive Datasets (Leskovec, Rajaraman, Ullman): http://infolab.stanford.edu/~ullman/mmds/book.pdf\n",
    "\n",
    "# Other Links\n",
    "- Impatient R, http://www.burns-stat.com/documents/tutorials/impatient-r/\n",
    "- R: The Good Parts, http://blog.datascienceretreat.com/post/69789735503/r-the-good-parts\n",
    "- Two excellent textbooks, freely available as PDFs:\n",
    "  - ISLR (Intro. to Statistical Learning in R): http://www-bcf.usc.edu/~gareth/ISL/\n",
    "  - ESL (Elements of Statistical Learning): http://statweb.stanford.edu/~tibs/ElemStatLearn/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
